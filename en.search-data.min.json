[{"id":0,"href":"/Front-End/","title":"Front End","parent":"","content":""},{"id":1,"href":"/Back-End/","title":"Back End","parent":"","content":" How the back end works _v 2.0     By Sam Mazarei All previous versions have been deprecated[.](I waited my whole life to type that)\nIt begins in the front end\u0026hellip;\nWhen a student submission is generated, two files are uploaded as objects to the submitter S3 bucket. One is a single .py file provided by the student(s), and the other a .json file generated by the front end logic containing all the requisite data to properly process the submission.\nThis json file will contain the following:\n submissionid : a 128 bit MD5 Hash unique to the specific students, admin, event, and problem. admin : The admin\u0026rsquo;s unique identifier (e.g \u0026ldquo;m.soltys\u0026rdquo;). event : A string identifier that is unique to the admin. problem : A string identifier unique to the event. tokens : A(n) list/array of tokens generated by the front end upon event registration.  This example would be named /admin/event/assignment/\u0026lt;subid\u0026gt;/\u0026lt;subid\u0026gt;.json , where the subid is calculated at the front-end during submission but can be found here as submission_example.json { \u0026quot;subid\u0026quot;: \u0026quot;fc55c0190dde2bc413d8d1e79fb8cca2\u0026quot;, \u0026quot;admin\u0026quot;: \u0026quot;m.soltys\u0026quot;, \u0026quot;event\u0026quot;: \u0026quot;aws_labs\u0026quot;, \u0026quot;assignment\u0026quot;: \u0026quot;lab5_containers\u0026quot;, \u0026quot;tokens\u0026quot;: [ \u0026quot;d823640ab3b0f7a4a2bc9fc89661e940\u0026quot;, \u0026quot;240669d4326dea48bba75e066b90b76f\u0026quot;, \u0026quot;4b31d568d86a9350d746c7c2fe9bf5c8\u0026quot; ] } S3 buckets only store objects, not files, but a pseudo file system can be achieved by using full paths as keys according to a pseudo file system hierarchy. Using values from the above example, this file and the student submission would be uploaded as:\n/m.soltys/aws_labs/lab5_containers/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.py and\n/m.soltys/aws_labs/lab5_containers/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.json Furthermore, at the time of event/assignment creation by an admin, data relevant to the assignment such as the test and grade case in/out files are also uploaded to the S3 bucket. By the time the example submission from above is submitted, test and grade case files exist in the bucket as:\n/m.soltys/aws_labs/lab5_containers/grading/test.in /m.soltys/aws_labs/lab5_containers/grading/test.out /m.soltys/aws_labs/lab5_containers/grading/grade.in /m.soltys/aws_labs/lab5_containers/grading/grade.out Every assignment in every event will contain a pseudo directory called grading. Again, this directory does not exist but is part of the naming convention. Test and grade case in and out files will be found in the grading pseudo directory.\nThese files are provided by the admin, and are formatted such that each line is an input, and for each input the student submission is executed with the input passed in as a command line argument. Ten test case inputs would result in both .in and .out files that are ten lines in size. .out files are the expected output when the respective line of input is passed to the student submission.\nAll of these files/objects will be used by the backend to process student submissions.\nA json upload to the S3 bucket triggers a lambda function\u0026hellip; (Not just new object creation!)     A json upload to the S3 bucket triggers a lambda function written in python. This lambda function generates a message whose body contains the key, value pair subdata: \u0026lt;key of object triggering lambda\u0026gt; and places it on the SQS queue. This message is a json object maded up of many other key, value pairs. This is an example of a message body for example above:\n{ \u0026quot;subdata\u0026quot; : \u0026quot;/m.soltys/aws_labs/lab5_containers/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.json\u0026quot; } \u0026hellip;once this message arrives to the queue\u0026hellip;\nThe back end makes its inglorious entrance\u0026hellip;     The back end\u0026rsquo;s processStudentsTestCases.py constantly polls the SQS queue for a message and when one is returned a client thread is spawned with the message passed in to its constructor. Then the main thread calls start() on this client thread, which in turn calls the client thread\u0026rsquo;s run() method, then begins polling the SQS queue for the next message.\nClient Thread     When the client thread\u0026rsquo;s constructor executes, it populates all instance variables except the student email list. This is done later in the process. The entry point for all client thread\u0026rsquo;s is run(), and when the thread executes it begins by\n Creating a directory using the subid from the subdata filename, for example  /home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/ It is important to check if this directory already exists, implying a previous usbmission is still being graded. In this case the thread quietly exits and the current submission is processed at a later time.   Changing present working directories into the newly cerated directory, from there retrieving both the newly submitted json and python files from the S3 bucket, for example:  /home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.json /home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.py    Retrieve test case in/out from S3 bucket     From the data in the json file, our script will then\n download the test inputs and outputs, test.in and test.out from the S3 bucket.  The directory structure at this point:\n /home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.json /home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.py /home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/test.in /home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/test.out  Execute submitted code in Python 3.9 container     At this point we have the code to execute, the inputs to run, and the appropriate outputs for comparison. We are using a python 3 docker image with a custom, executable script executeSubmission.py installed that will execute the submitted code against all provided inputs from test.in, generating and generating the output file. Our script will now:\n Use a subprocess that will in turn execute the appropriate docker run command.  docker run -it --rm --name=\u0026quot;subidfc55c0190dde2bc413d8d1e79fb8cca2\u0026quot; -v \u0026quot;$PWD\u0026quot;:/usr/src/submitter -w /usr/src/submitter python:3.9 executeSubmission.py fc55c0190dde2bc413d8d1e79fb8cca2    This executes our container as a subprocess and keeps it alive until it finishes executing. The present working directory gets bind-mounted to a newly created directory in the container /usr/src/submitter. Read more on the container and how it works here . Since the default for bind mount is read-write, this allows our script in the container to generate an output that can be read by our main thread.\nReturn output to the front-end via submitter S3 bucket and clean house     At the time of this writing it is understood that the way to return the output generated by the user\u0026rsquo;s submission is for the backend to upload the newly generated fc55c0190dde2bc413d8d1e79fb8cca2.result to the submitter S3 bucket for the front end to retrieve.\nNow that our worker thread has finished executing, our script\u0026rsquo;s current directory should contain:\n /home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.json /home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.py /home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/test.in /home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/test.out /home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/fc55c0190dde2bc413d8d1e79fb8cca2.out  Our script will now:\n upload fc55c0190dde2bc413d8d1e79fb8cca2.out to submitter S3 bucket. change working directories up one to /home/ec2-user/. delete the directory /home/ec2-user/fc55c0190dde2bc413d8d1e79fb8cca2/  Finally, now that the submission has been processed, the working space has been cleaned, and the output generated has been sent to the next destination, our script must:\n delete the message that was originally retrieved from the submitter SQS queue.  It is important to remember that when our script initially retrieved the message from the SQS queue, that it doers not remove the message from the queue, it only hides it from any other process being able to retrieve it for the ammount of time defined in the parameters for the SQS instance.\nFuture work     We want to use SES to email the results of test cases to the students after uploading results to S3 bucket.\n"},{"id":2,"href":"/Infrastructure/","title":"Infrastructure","parent":"","content":" AWS Infrastructure Doc      AWS Infrastructure Doc Compute  EC2  AMI Lambda Function - Relational Database Service -     Networking  VPC Subnet Configuration Internet Gateway NAT Gateway Load Balancer   Security Group Communication SQS SES S3 IAM Cloud Formation   Compute      EC2     Names of Current EC2 instances ↕   Web Server App Server Bastion Host    AMI     Author: Arthur Hui AMIs are snapshot of the instances at that very moment in time. They contain all of the files that exist in the instance at that very moment. All operating system updates, installed libraries, and stuff will be there when it is recreated and turned into an EC2 instance.\nAMIs ↕   App Server AMI: Has docker, MySQL, and python installed on it. Web Server AMI: Has MySQL, python, Django, and apache installed. There are\nalso html and python files to host the actual website.    Lambda Function -     Author: Edgar Ramirez\n The AWS Lambda service creates what\u0026rsquo;s called Lambda functions which help in calling code after a certain event has occured in the application. The code can also call other AWS services, such as a load balancer, SQS or SNS, EC2, or S3 Bucket.   Our Lambda function gets triggers when AWS receives an object in the S3 bucket (an object storage service), in this case when admin creates a new event through the webpage.\n When the event gets creates the information entered by the user gets saved into a JSON file and saved onto the S3 Bucket.   The Lambda function, now triggered, will run code (Python) to call AWS SQS (Simple Queue Service, sends messages)  It will query a list of emails (from RDS, AWS Relational Database Service) of the students that will be participating in the assignment. Will email the list of students their submission tokens for the assignment.        Relational Database Service -     Author: Arthur Hui\nAmazon Relational Database Service is how we are storing information that the front end and back end need to access. The database use MySQL to handle queries, updates, deletions, and creation of data inside the it.\n Here is the Entity Relation Diagram. Shows what fields that each entity has associated to it and how they associated with one another.    Steps to connect to the RDS from the instance\n Find the user name of the RDS you want to connect to Find the password for the RDS you want to connect to Get the endpoint of the RDS you want to connect to Use the command below to access the database.  Replace the #### with the RDS endpoint and $$$$ with the user name of the RDS    mysql -h ####### -P 3306 -u $$$$$$ -p   Networking      VPC     VPC configuration ↕   CIDR-10.0.0.0/16 Virtual Private Cloud Will encapsulate the following resources  Public Subnet Private Subnet Internet Gateway Nat Gateway Public Route table: Associated with IG Private Route table: Associated with Nat Gateway      Subnet Configuration     Author: Jose Cahue\nPublic Subnet ↕   CIDR- 10.0.0.0/24 Public to the public internet Will encasulate the following resources  Bastion Host- Manages access to Web server Web Server- Responsible for hosting website Nat Instance-Responsible for forwarding traffic to App server.      Private Subnet ↕   CIDR- 10.0.0.0/24 Will encapsulate the following resources  App Server-Responsible for back end logic processing Docker deamon-Service used to decouple diffrent process functionality      Internet Gateway     Author: Jose Cahue\nRole ↕  -Responsible for public internet access into the VPC   NAT Gateway     Author: Jose Cahue\nRole ↕  -Responsible for managing communication internal VPC data traffice to our private subnet resources   Load Balancer     Author: Edgar Ramirez\n The AWS Load Balancer spreads out application traffic to various items such as EC2 instances. (EC2 instances, containters, IP Addresses, Lambda functions, and etc.)  Single or Multiple Availability Zones  The application can choose to reroute the overflow of traffic to just one or multiple availability zones or a section of the globe that provides resources, computing or storage or both for the application. This project has two load balancers  Both use multiple availability zones  US-East-1a (Virginia) US-East-1b (Virginia)          Security Group Communication      Author: Arthur Hui\nThe security groups allows for network communication between the instances, RDS, and the internet. Each security group should be explicitly name to avoid confusion. Here are the security groups that are being used in our infrastructure.\nSecurity Groups ↕   Bastion Host SG  Description: allow ssh into bastion host Inbound rule: ssh on port 22 with source 0.0.0.0/0 Outbound rule: All traffic on all ports with 0.0.0.0/0   Web Server SG  Description: allow ssh from bastion host into Web Server and people to view the sight Inbound rules  Type ssh on port 22 with the source being the Bastion Host SG Type HTTP on port 80 with source being 0.0.0.0/0 and ::/0 Type HTTPs on port 443 with source being 0.0.0.0/0 and ::/0 Type custom tcp on port 8000 with source being 0.0.0.0/0   Outbound rule: All traffic on all ports with 0.0.0.0/0   App Server SG  Description: allow ssh from bastion host into App Server Inbound Rules  Type ssh on port 22 with the source being the Bastion Host SG Type MYSQL/Aurora on port 3306 with source being 0.0.0.0/0   Outbound rule: All traffic on all ports with 0.0.0.0/0   RDS_SG  Description: allow RDS to be accessed by app and web server Inbound rule: MYSQL/Aurora on port 3306 with source being 10.0.0.0/16 Outbound rule: All traffic on all ports with 0.0.0.0/0      SQS     Author: Zhili Wang\nAmazon Simple queue service (SQS) is a fully managed message queue service for communicating between distributed systems and microservices at any scale. SQS makes it simple to decouple the application component so it can run and fail independently. This allows users to build an application that is fault tolerant and easy to scale.\n  Queues ↕  We have two standard SQS Queues, both are trigger by the Lamda function base on its invocation condition.   Queue type ↕  We used the standard Queue for simplicity because the order of the message is not important.   Queue Details ↕  Our queue accept data size between 1 to 256 kb, and the message retention period is 4 days. There is a 10-second delay on delivery and receive wait time.   Queue Usage ↕    SES     Author: Elijah Clark\n Simple Email Service is used to send out emails via AWS. Allows for emails to be sent to verified emails from AWS console. And if SES setting for sandbox mode is set to off then one may send out emails to unverified emails..\nPrerequisites ↕    DOMAIN\nA domain must be set up either with AWS route 53. Or created using a 3rd party Domain service. Using route 53 is well documented, thus follow documentation if going with AWS made domain.\n  EMAIL\nAn email can be verified inside of SES console under Identity management, and simply clicking Verify a new email address one may enter an email that they want to send emails from or to.\n  aws_access_id \u0026amp; aws_secret_access_key\nAWS has this well documented, must have admin generate and give these values.\n  Toggle Sandbox mode off\nBy going to SES console, under the sending statistics tab one may click edit your account details to enable production access, fill in the rest of the needed information and submit for review. Wait for verification to finish. Once done with the review one can finally send emails via code to unverified emails.\n    Components ↕    Region\nUsing AWS region format, example: \u0026ldquo;us-east-1\u0026rdquo;. Is used to set region currently being used for by AWS service.\n  Domain\nDomain that was from prerequisites is used as input during use of constructor or simply a call to the domain setter prior to use of method to send emails via domain.\n  Recipient\nEmail that was from prerequisites is used as input during use of constructor or simply a call to the recipient setter prior to use of method to send emails via domain Once outside of sandbox mode SES can send emails out to unverified emails, thus sending emails to any address that is valid.\n  Subject\nSimply a string for setting subject of email\n  Body_text\nSimply a string input for body of text for body of email.\n  Body_html\nMust have correct formatting and be correct HTML otherwise it will not work. Recommend testing html separate before passing it in as a string. All html must be passed as a string.\n    USE ↕  Import SES.py into another PY file.\nOnce imported make a call to constructor and use setters from SES.py to set the values for domain, region, recipient, html, and subject. Once all needed information is there to make up to totality of the email. simply call method send email with domain which will use provided information to send email out.\n  S3     Author: Zhili Wang\n Amazon Simple Storage Service (S3) is a cloud storage that offers scalability, data availability, security, and performance. It has a simple web service interface that can be used to store and retrieve any amount of data at any time from anywhere. The S3 bucket is also cost-effective and very reliable in terms of preventing data loss.\n Property ↕  The bucket region is us-east-1, and the event notification destination is our lambda function (Submitter_ Trigger).   Permission ↕  We use Identity and Access Management (IAM) and bucket policy to control the permission of the S3 bucket   Usage ↕  Our s3 bucket can send a notification to the lambda function and retrieve user input (JSON File) from it. We can also upload the item to the bucket using Amazon SDK    Management ↕  We don\u0026rsquo;t have any rules for bucket management at the moment. We thought about making a lifecycle rule so it can transition objects to archive or delete the infrequent access data when they are no longer need it, but it\u0026rsquo;s not necessary with the current build of the project.   Storage ↕  Back end will explain how the objects are being stored in the S3   IAM     Author: Arthur Hui\n Identity and access management is useful for allowing multiple people access to the same AWS account. Each new IAM user must have certain polices attached to the user for accessing AWS resources. The owner can also put multiple users in a group and attach polices to said group to make IAM management easier.\n IAM naming examples ↕  First Names (using common names as an example) ↕   Cameron Kim Derek Bob    A specific user names ↕   Comp350ProjectLeader FrontEnd213 BackEnd456 Infrastructure805    Group naming ↕  - front-end-team - back-end-team - infrastructure-team      Roles are useful to allow certain resources to interact with each other like S3 to SQS and SQS to EC2 as an example. It also makes sure that these resources can only access those specific resources.\nExisting Roles ↕    Submitter_lambda_role\nallows the lambda function to get a objects from S3 and put a message on the SQS\n  ReadS3Bucket\nallows reading from the S3 bucket, SQS, and executing the SQS\n  rds-monitoring-role\n does what it says, monitors the relation database service    backend_queueaccess_role\n allows the back app server to receive, delete, and get attribute from the SQS      Cloud Formation     Author: Daniel Thai\n A service that helps users model and set up AWS resources for them to focus on their applications that run in AWS. Users could create a template that\u0026rsquo;ll describe all the AWS resources that they want to use and Cloud Formation will take care of provisioning and configuring the resources for the users.\n  Creating the template\n  Set up and log into your AWS account\nSign in to the Console\n  Define Access Controls\nSet up the access with AWS Identity\n  Deploy your first collection of resources\nCreate and deploy your first Cloud Formation template\n    YAML file template reference\nUsage:\nSubmitter Networking Enviroment and Resources\nThis template will:\n Create a VPC with:  2 Public Subnets      YAML file template\n   Resources: SubmitterVPC: Type: 'AWS::EC2::VPC' Properties: CidrBlock: 10.0.0.0/16 Metadata: "},{"id":3,"href":"/CloudFormationDoc/","title":"Cloud Formation Doc","parent":"","content":""},{"id":4,"href":"/FutureFeatures/","title":"Future Features","parent":"","content":""},{"id":5,"href":"/","title":"","parent":"","content":" Submitter Overview     Author: Alex Sullivan\nlast updated: 4/4/2021\n Submitter Overview  Description Front-End  Administrators Students   Back End     The submitter is a web application implemented With AWS cloud services, designed to work as an automatic grader for python programming assignments. Designed and built in Spring 2021, for Software Engineering (Comp 350) at California State University Channel Islands, with Professor Soltys.\nDescription     The submitter project consists, of a website used by administrators to create events, either for a class or a competitive coding competition. Students, or participants can then submit answers to various assignments assoicated with an event. Admins can create an account, in order to create, and modify events, as well as monitor student submissions.\nStudents can submit their code via the web site where it is passed to a backend server and run against pre determined test cases provided by the event admin. Students are given immediate feedback through email detailing the results of the test cases, grade cases are hidden from students until the end of the event.\nFront-End     The front end consists of a website hosted on an AWS ec2 instance, we decided to use django as our framework of choice for the webserver.\nAdministrators     Administrators have the following abilities within the front end interface:\n Create an event Modify an event or assoicated assignments View student submission grades  Event Creation\nWhen an admin creates an event the following information is given:\n Event Name Due date student email addresses Team size Assignment info  Assignment Due date Test case input, and expected output Grade case input and expected output    When an assignment is created, a unique token is created for each student for each assignment, thus if there are 5 assigments within the event then each student will recieve 5 separate tokens to submit there work.\nEvent Modification\nWhen an admin is trying to modify an event they will need to log in, then they will be able to view there open events. When an event is chosen, fields will poulate with the current event information. The fields can be changed and then submitted to the RDS.\nStudents     Students have very limited abilities, and do not even require an account. Via the frontend students can only do the following:\n Submit token(s) and a .py python file for each event they are enrolled in  Student Submission\nStudents, can submit code via a unique token for each assignment in which they are associated with. Students, can work alone or in groups depending on the event rules. If a student is working within a group one student can submit with all of their group members tokens. If they are working alone, then the student can submit with just their own token.\nOnce the tokens are submitted the RDS is checked to make sure that all of the tokens submitted are associated with the same assignment. If the tokens validated successfully, then the students can submit their .py file. When the python file is submitted, a submission id is generated along with a json file containing:\n Submission ID admin event problem Tokens  Both the submission json and the python file are placed within an S3 bucket at which point the back end takes over.\nClick here for more info Back End     Click here for a detailed overview of the backend The back end consists of several major parts and AWS serviced including the following:\n AWS Lambda function AWS SQS AWS SNS ec2 instance  Rather than explain each of these services in isloation the backend makes more sense described as a chronological sequence of events after a submission from the front end. Once the submission is placed within the S3 bucket an AWS lambda function is triggered. This generates a json message for the sqs queue.\nOnce Within the SQS queue, that is when our own library of code comes in. The EC2 instance pulls from the message from the queue, and retrieves the associated submission.json and the students code that goes along with that submission.\nTest inputs and there expected outputs are then queried from the database, and files used for comparison are generated. For example we may create a test.in for input to the program and a test.out to compare the student programs output with.\nAll of the execution of student code, happens with a docker container in order to sandbox student code. As running unsanitized code directly within an AWS is a large security risk.\nOnce the student code has finished running and the output has been written to a file, the output is placed back into the S3 bucket. That file can at that point be delivered via email to all students who\u0026rsquo;s tokens are associated with the submission or, we may try to recieve the output on the front end to display via the web app.\n"},{"id":6,"href":"/Back-End/DockerTest/","title":"Docker Test","parent":"Back End","content":"This will describe how we test the functionality of our custom container and subexecutor script. These tests are to be very basic and treated like unit tests to ensure that we have the functionallity down for deployment and future tests.\nTest Files     The files used for testing are:\n test.in test.out \u0026lt;MD5\u0026gt;.py  Test 1     We are simulating an event assignment in which the admin wants students to write a python program that takes any number of integers from the command line and sums them together. The output should be a single integer value.\nThe file test.in contains a single test case on each line. Each test case consists of any number of integers.\nEach line in the file test.out consists of a single integer value. When a line from test.in is passed into the student\u0026rsquo;s code, the expected return value is on the corresponding line number in test.out.\nWe will use \u0026lt;MD5\u0026gt; = 612a6ba796901572fb0fd23842062549 for our subID.\n612a6ba796901572fb0fd23842062549.py is a student submission and should generate an output file 612a6ba796901572fb0fd23842062549.out.\nTest 2     We are simulating an event assignment in which the admin wants students to print a word provided as a command line argument. The output should be a single string.\nThe file test.in contains a single test case on each line. Each test case consists of a single string.\nEach line in the file test.out is identical to test.in. When a line from test.in is passed into the student\u0026rsquo;s code, the expected return value is on the corresponding line number in test.out.\nWe will use \u0026lt;MD5\u0026gt; = 328b426862813868adb62fad0c5a165f for our subID.\n328b426862813868adb62fad0c5a165f.py is a student submission and should generate an output file 328b426862813868adb62fad0c5a165f.out.\nHow the test will be run      We must manually set up the environment for the container, but in production a daemon will be preparing the environment and executing the docker run command. This means we must create a directory on the host at /usr/src/submitter/\u0026lt;MD5\u0026gt;/ and move all of the test files into it. Execute the docker run command.  What a successful result looks like     Upon completion of the docker run command, there will be a file /usr/src/submitter/\u0026lt;MD5\u0026gt;/\u0026lt;MD5\u0026gt;.out on the host. It is a csv file that lists the input, expected output, the actual output, and pass/fail on each line according to each test case.\nExecuting Test 1     Simulated directory layout     ec2-user@ip-10-0-1-230:~/testing/testFiles/dockerbuild/612a6ba796901572fb0fd23842062549$ ll total 12 drwxrwxr-x 2 ec2-user ec2-user 80 Apr 5 09:38 ./ drwxrwxr-x 4 ec2-user ec2-user 157 Apr 5 08:56 ../ -rw-r--r-- 1 ec2-user ec2-user 287 Apr 5 09:08 612a6ba796901572fb0fd23842062549.py -rw-r--r-- 1 ec2-user ec2-user 63 Apr 5 08:11 test.in -rw-r--r-- 1 ec2-user ec2-user 22 Apr 5 08:11 test.out  Docker run     ec2-user@ip-10-0-1-230:~/testing/testFiles/dockerbuild/612a6ba796901572fb0fd23842062549$ docker run -it --rm --name=\u0026#34;MD5\u0026#34; -v \u0026#34;$PWD\u0026#34;:/usr/src/submitter -w /usr/src/submitter subexecutor executeSubmission.py 612a6ba796901572fb0fd23842062549  Results     ec2-user@ip-10-0-1-230:~/testing/testFiles/dockerbuild/612a6ba796901572fb0fd23842062549$ lltotal 16 drwxrwxr-x 2 ec2-user ec2-user 124 Apr 5 09:38 ./ drwxrwxr-x 4 ec2-user ec2-user 157 Apr 5 08:56 ../ -rw-r--r-- 1 root root 174 Apr 5 09:38 612a6ba796901572fb0fd23842062549.out -rw-r--r-- 1 ec2-user ec2-user 287 Apr 5 09:08 612a6ba796901572fb0fd23842062549.py -rw-r--r-- 1 ec2-user ec2-user 63 Apr 5 08:11 test.in -rw-r--r-- 1 ec2-user ec2-user 22 Apr 5 08:11 test.out Output report\nec2-user@ip-10-0-1-230:~/testing/testFiles/dockerbuild/612a6ba796901572fb0fd23842062549$ cat 612a6ba796901572fb0fd23842062549.out input,expected output,actual output,pass/fail 1 2 3,6,6,Pass 1 2 3 6,12,12,Pass 90000 5000 4000 1000,100000,100000,Pass 400 50 6,456,456,Pass Success!\nExecuting Test 2     Simulated directory layout     ec2-user@ip-10-0-1-230:~/testing/testFiles/dockerbuild/328b426862813868adb62fad0c5a165f$ ll total 12 drwxrwxr-x 2 ec2-user ec2-user 80 Apr 5 09:41 ./ drwxrwxr-x 4 ec2-user ec2-user 157 Apr 5 08:56 ../ -rw-r--r-- 1 ec2-user ec2-user 220 Apr 5 09:08 328b426862813868adb62fad0c5a165f.py -rw-r--r-- 1 ec2-user ec2-user 31 Apr 5 06:41 test.in -rw-r--r-- 1 ec2-user ec2-user 31 Apr 5 06:41 test.out  Docker run     ec2-user@ip-10-0-1-230:~/testing/testFiles/dockerbuild/328b426862813868adb62fad0c5a165f$ docker run -it --rm --name=\u0026#34;MD5\u0026#34; -v \u0026#34;$PWD\u0026#34;:/usr/src/submitter -w /usr/src/submitter subexecutor executeSubmission.py 328b426862813868adb62fad0c5a165f  Results     ec2-user@ip-10-0-1-230:~/testing/testFiles/dockerbuild/328b426862813868adb62fad0c5a165f$ ll total 16 drwxrwxr-x 2 ec2-user ec2-user 124 Apr 5 09:42 ./ drwxrwxr-x 4 ec2-user ec2-user 157 Apr 5 08:56 ../ -rw-r--r-- 1 root root 155 Apr 5 09:42 328b426862813868adb62fad0c5a165f.out -rw-r--r-- 1 ec2-user ec2-user 220 Apr 5 09:08 328b426862813868adb62fad0c5a165f.py -rw-r--r-- 1 ec2-user ec2-user 31 Apr 5 06:41 test.in -rw-r--r-- 1 ec2-user ec2-user 31 Apr 5 06:41 test.out Output report\nec2-user@ip-10-0-1-230:~/testing/testFiles/dockerbuild/328b426862813868adb62fad0c5a165f$ cat 328b426862813868adb62fad0c5a165f.out input,expected output,actual output,pass/fail hello,hello,hello,Pass nice_to_meet_you,nice_to_meet_you,nice_to_meet_you,Pass pardon,pardon,pardon,Pass Success!\n"},{"id":7,"href":"/Back-End/ExampleSubmissionJSON/","title":"Example Submission JSON","parent":"Back End","content":"When a student submits code via the front end web interface, a similar json file is created as shown below, and stored within the s3 bucket.\n{ \u0026quot;submissionid\u0026quot;: \u0026quot;fc55c0190dde2bc413d8d1e79fb8cca2\u0026quot;, \u0026quot;admin\u0026quot;: \u0026quot;m.soltys\u0026quot;, \u0026quot;event\u0026quot;: \u0026quot;aws labs\u0026quot;, \u0026quot;problem\u0026quot;: \u0026quot;lab 5 - containers\u0026quot;, \u0026quot;tokens\u0026quot;: [ \u0026quot;d823640ab3b0f7a4a2bc9fc89661e940\u0026quot;, \u0026quot;240669d4326dea48bba75e066b90b76f\u0026quot;, \u0026quot;4b31d568d86a9350d746c7c2fe9bf5c8\u0026quot; ] } "},{"id":8,"href":"/Front-End/Django/","title":"Django","parent":"Front End","content":"Django is the front end web framework that we chose to write the submitter website with. Django is written in python and makes handling server side requests very easy. To learn more about Django I suggest you go throught the tutorial provided in the django documentation linked here . This will take some time however, it will greatly help your understanding of the submitter website.\n"},{"id":9,"href":"/Front-End/Django/models.py/","title":"Models.Py","parent":"Django","content":""},{"id":10,"href":"/Front-End/Django/Views.py/","title":"Views.Py","parent":"Django","content":"Views within django are how the web server handles a response when a user visits on of the URL\u0026rsquo;s. Each URL for the submitter website has a corresponding view, which can handle, POST or GET requests and respond with an HTML response.\n"},{"id":11,"href":"/Front-End/Website/","title":"Website","parent":"Front End","content":""},{"id":12,"href":"/Front-End/Website/CreateEvent/","title":"Create Event","parent":"Website","content":" Migrate .md file     "},{"id":13,"href":"/Front-End/Website/ModifyEvent/","title":"Modify Event","parent":"Website","content":" migrate data     "},{"id":14,"href":"/Front-End/Website/StudentSubmit/","title":"Student Submit","parent":"Website","content":"Author: Alex Sullivan\nLast Update: 4/4/2021\nA student can submit python source code for an assignment via the front end web interface. Students submit one token for each member in their group. When all tokens are submitted, then they need to be validated.\nValidation refers to making sure that all of the tokens submitted are associated with the same assignment. We allow very free form groups, and dont try to keep track of who is in which group. All that we try to validate is that the number of tokens is within the specified group size, and that all tokens are associated with the same assignment. We dont want a student to submit the wrong token and have grades mixed for different assignments.\nOnce the tokens are validated, then the students are allowed to submit their python source code for the assignment. At which point a json file is created for the submission, and the .py student submission is placed within the s3 bucket along with the submission.json. For more information about the json submission follow this link .\n"},{"id":15,"href":"/Tutorials/","title":"Tutorials","parent":"","content":""},{"id":16,"href":"/Tutorials/Sams-SSH-Article/","title":"Sams SSH Article","parent":"Tutorials","content":" SSHelling into the VPC through the Bastion     Hopefully this will take some of the guess work out of using SSH to access resources on the VPC through the bastion. I will demonstrate how to add the bastion key with an additional key for some resource in our cloud. Any resource we would like to access will have an associated key and we will need that key to ssh into it. The instructions scale, so it is no different for 3 or 9000+ keys.\nSanity check     First you will want to make sure that\n You have ssh up and running on your machine. I believe a quick\nsudo apt install ssh-server\nwill do the trick on Ubuntu. You have the appropriate keys in a directory somewhere that you can access on your local machine (I keep them in the directory for COMP350). Make sure your keys are appropriatly named. I will be using the Bastion.pem, and Webapp.pem keys.  You got all that?     Let\u0026rsquo;s get to it. (TL;DR Scroll to the bottom)\n  You need to open a terminal and change directory to directory containg the .pem keys\n$ cd \u0026lt;DirectoryWithPEMFiles\u0026gt;\n  We need to change the permissions of both keys. Since these are security keys meant to authenticate, it makes no sense to allow anyone else any priveleged access (and ssh will not use a key if it doen\u0026rsquo;t maintain the correct priveleges). In Linux there are 3 types of user for a file or directory- the file owner, the file\u0026rsquo;s group, and then the rest of the world. We only want the owner (us) to access, so for this we will use 400. That is read access only for the owner.\n$ chmod 400 Bastion.pem $ chmod 400 WebApp.pem\nor if you think you\u0026rsquo;re slick try\n$ chmod 400 *.pem\n(hackerman!)\n  Now we need to add these keys to our agent. First take a look at what is already there\n$ ssh-add -L\nThis will list the keys the agent is holding.\n  Now add the keys we need\n$ ssh-add -k Bastion.pem\n$ ssh-add -k WebApp.pem\n  Take a look so you know what it looks like to add a key\n$ ssh-add -L\n  Now we SSHell in\n$ ssh-add -A ec2-user@theIPOfTheBastionHost There\u0026rsquo;s a couple things to note here - first, the -A. This tells the Bastion and SSH that we have other keys we want to use. The second, ec2-user is not a filler. That is the default login on all instances.\n  You may be prmopted with a warning, accept it. You see it every time you SSH into a machine for the first time.\n  From the bastion host\n$ ssh ec2-user@theIPOfTheWebApp\nYou will see the same warning from earlier, reply Y.\nYou have experince the SSHell.\n  TL;DR\n$ cd \u0026lt;DirectoryWithPEMFiles\u0026gt;\n$ chmod 400 Bastion.pem\n$ chmod 400 WebApp.pem\n$ ssh-add -L\n$ ssh-add -k Bastion.pem $ ssh-add -k WebApp.pem\n$ ssh-add -L\n$ ssh-add -A ec2-user@theIPOfTheBastionHost\n$ ssh ec2-user@theIPOfTheWebApp\n"},{"id":17,"href":"/categories/","title":"Categories","parent":"","content":""},{"id":18,"href":"/Tutorials/Understanding-Git-and-Git-Hub/","title":"Git and GitHub","parent":"Tutorials","content":"    Git and GitHub are not the same thing  Git GitHub   Lets Start With the Basics of Git  git init git add git commit, git log git reset, git checkout git revert   git branch  Creating a branch Merging   GitHub  Forking git clone git push       Git and GitHub are not the same thing     First lets get into what git and GitHub are and what the differences are.\nGit     Git is a command line version control software that lets you track changes in your code through something called commits. Say you are working on a project, and you get to a point where your code works but you want to implement more features. You may want to save your project at this state, so you can go back to it. Well with git, you can make something called a commit that saves your work at that point.\nThen you can go on and continue with your project and keep implementing features and committing the changes as you successfully complete features. However you\u0026rsquo;ve reached a point where the feature you\u0026rsquo;ve implemented breaks your program, and you want to go back to when your project worked. This is the beauty of git, you can revert back to one of your previous commits. There are other features within git as we will git into later.\nGitHub     Github is for collaboration between developers.\nLets take the previous scenario where you are working on your project and you decide you need help. You can email all your source code to everyone else you want to work with, OR you can upload to GitHub and allow anyone who wants to add features to your project to clone your code on their own machines at home.\nLets Start With the Basics of Git     First make sure you have Git, This guide is written for those using Linux or Mac OS. If you\u0026rsquo;re on windows you will need to download git, but the commands will be the same.\nCheck what version you have with:\nGit --version If you get an output with a version number you have git installed, on mac if your don\u0026rsquo;t have git already it will prompt you to install it. On Linux you will need to use sudo apt-get install git all or use your package manager for your given distro.\ngit init     Before you can do anything with git, you have to initialize the repository. Lets make a fake project.\nIn this picture you can see that I have created a project that lives in the directory my-git-project with one python file that prints hello world., and I have initialized the repository. Git can now be used within this folder and will track the changes to all files within the directory.\ngit add     Now that we have the repository set-up we can start adding files to the staging area. When using git, every-time you make changes to a file git tracks those changes, and before we can make a commit we mst put the files into the staging area with git add \nIn this picture you can see that I added the helloWorld.py to the staging area with git add * the * just means all the files that have been edited since the last commit or in this case since I initialized.\nThen I used the command git status to see what files were in the staging area and you can see that helloWorld.py is not in the staging area. However after I created the newFile.py and used git status again you can see that newFile.py is NOT in the staging area because I did not use git add  to place it in the staging area.\nNotes:\n You don\u0026rsquo;t need to add all files to the staging area, there are other options that can be used to add single files to the staging area with git add \u0026lt;filename\u0026gt; to remove files from the staging area use git rm  git commit, git log     git commit is the bread and butter of git. This basically lets us take a \u0026ldquo;snapshot\u0026rdquo; of our code at that moment of time. kind of like saving but we can revert back to this save spot later. lets use the command git commit -m  to commit our code to the master branch.\nnote: the -m stands for message, this allows us to attach a message to the commit, try to be descriptive with your messages.\nAbove I committed the helloWorld.py , then added newFile.py to the staging area and committed that file as well.\nNow Ive added some code to newFile.py and committed the changes , but lets say that for some reason that I didn\u0026rsquo;t want these changes anymore, because Ive decided to take the project in a new direction and I know that at the second commit I had a clean newFile.py\ngit reset, git checkout     Notice in the picture above, each commit has a long string of characters next to it. These are unique hashes for each of our commits, you can think of them like nodes along the chain representing a previous \u0026ldquo;save\u0026rdquo; state. Say we wanted to get rid of the code that we wrote in the newFile.py. To do that first we need to a git reset\nNotice that when we do a git reset, we use the hash of the commit that we want to move to. However we need to do one more thing. If you were to open up newFile.py you might expect an empty folder with no code, however this is not the case, when I open up the new file my code still looks like this:\nprint(\u0026#34;is a new file\u0026#34;) print(\u0026#34;im gonna remove these changes\u0026#34;)  git reset can be a dangerous command because it completely gets rid of the all the commits after the one you reset it to. This is why it does not change the files, or in this case why the lines of code were still in newFile.py after it was reset. For example say you did the reset on accident or reset to the wrong commit, you could git ad . then git commit -m to not completely lose the code you wrote. To actually change the files within your working directory keep reading.  We need one more step to change the files in the working directory, the git checkout, a couple things to note about git checkout here:\n  After this all the changes to newFile.py were gone and because it was an empty file before the commit, its empty again now\n  Notice that I used the hash that the HEAD was currently out\n  I specify what files I wanted to change with -- newFile.py, if you forget this you will get into a detached head state which can be hard to fix\n  NOTE: to do both of the above in one step do git resest \u0026lt;hash\u0026gt; --hard . More dangerous this way but faster.\ngit revert     git revert is another way to revert your code back to the following. Im gonna make this part quick, heres the changes i made:\n  helloWorld.py changed to look like this:\nfor i in range(5): print(\u0026#34;hello world\u0026#34;)   Committed changes to helloWorld.py\n  newFile.py looks like this:\nprint(\u0026#34;hey im new file\u0026#34;)   committed changes to newFile.py\n  my git log after all of the following now looks like this:\nlets revert to the 3fdcdd2 hash with git revert 3fdcdd2 and we have the following git log :\nNotice that the commits aren\u0026rsquo;t gone with revert, but instead another is added with the message \u0026ldquo;revert \u0026hellip;.\u0026rdquo; this is because we are simply undoing the commit at that hash so my files now look like this:\nnewFile.py\nprint(\u0026#34;hey im new file\u0026#34;) helloWorld.py\nprint(\u0026#34;hello world\u0026#34;) Notice that the changes I made to the newFile.py stayed while the changes I made to the helloWorld.py were undone. this\ngit branch     branching is where the real power of git really lies, This allows you to take your project in different directions without messing up other branches. Up until now we\u0026rsquo;ve been working with git in a very linear fashion, lets try branching.\nCreating a branch     use the following commands to create a branch and witch to the branch. Lets pretend we\u0026rsquo;ve had an idea for a new feature.\ngit branch myNewFeature\ngit checkout myNewFeature\nNow I have the following git log --oneline:\nyou can see here that the HEAD points at the new branch myNewFeature . Anything we did before on the master branch we can now do on our new branch. for example, lets create a new feature file.\nabove you can see that our new feature has been added to the new branch. If we like the new feature, and are ready to commit it to the master branch we can merge them together.\nMerging     Because we may have multiple branches we may want to see all of the branches currently open. To do that use git branch. Notice the star, that tells you what branch you are currently on.\nNow on to merging myNewFeature with master. First you need to switch to the master branch, then merge the desired branch into master.\nYou will notice here that the myNewFeature branch is still there, you can either keep developing on that branch or delete it with git branch -d myNewFeature\nGitHub     Now lets move on to GitHub.\nThe core functionality of GitHub lies with its ability to allow you to fork other peoples repositories, and then start working on them on your own machine. were going to do a quick tutorial on how we can fork our submitter code.\nForking     Go to the submitter repository shown below.\nThen go ahead and click the fork button and choose to fork the project to your own personal machine. Ive already done so, which is why you can see the little one by the fork button in the top right.\nNow I need to go to my forked repository on my own personal GitHub account to clone the repo to my computer to get to work.\n notice the forks on the left menu, when we bring the repository onto our own machines all of these branches will exist.  git clone     There are multiple ways to clone a repository to your machine, I will show you how using git and the terminal.\nyou can see that I am on my personal GitHub looking at my forked version of the Comp350-Submitter repo, and notice that little link underneath clone, thats the link I am going to use with GitHub to clone the repository, shown below.\nBecause we cloned this repo from GitHub it is already a git repository, so there is no need to use git init. Cd into the project folder wherever you cloned it, and do a git branch -a\nnotice that all of the branches exist, but these are known as remote branches, you cant make commits to these branches. We need to make our own local branch, that tracks a remote branch. I am part of the front end team so I did the following:\nNow I have my own front end branch to work so lets try to make some changes.\ngit push     After committing these changes to my local front end branch I was able to push them to my forked version of the GitHub repository, on my personal GitHub account, with git push\nand here are the changes on the GitHub repo\n![githubshowing changes](/images/githubshowing changes.png)\nIf I am really happy with my changes I can go and create a pull request for my changes to be put into the official submitter repository. To do this got to the submitter repository on the organization and hit new pull request, and compare across forks. you should get a page that looks like this:\nI can then hit create pull request and someone will review my changes and either confirm or accept them. If they accept then the changes will be added into the organizations code base.\n"},{"id":19,"href":"/Tutorials/GitHub-Desktop/","title":"GitHub Desktop","parent":"Tutorials","content":"Alexander Tolley\nLink to the google Slides "},{"id":20,"href":"/tags/","title":"Tags","parent":"","content":""}]